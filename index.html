<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="robots" content="noindex">

    <title>Deniz Engin</title>
    <meta name="description" content="Deniz Engin | Personal Website">
    <meta name="author" content="Deniz Engin">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Deniz Engin">
    <meta name="twitter:description" content="Deniz Engin Twitter">
    <meta name="twitter:creator" content="Deniz Engin">


    <meta property="og:type" content="article">
    <meta property="og:title" content="Deniz Engin">
    <meta property="og:description" content="Deniz Engin | Personal Website">

    <meta name="msapplication-TileColor" content="#ffc40d">

    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="css/main.css?1589893399693513343">
    <link rel="canonical" href="http://localhost:4000">
    <link rel="alternate" type="application/rss+xml" title="Deniz Engin" href="feed.xml">
</head>


<body>
<span class="mobile btn-mobile-menu">
  <i class="icon icon-list btn-mobile-menu__icon"></i>
  <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
</span>

<header class="panel-cover panel-cover--collapsed left-bar" style="background-image: url(images/cover.jpg); ">
    <div class="panel-main">

        <div class="panel-main__inner panel-inverted">
            <div class="panel-main__content">
                <a href="" title="link to home of Deniz Engin">
                    <img src="images/profile.jpg" class="user-image" alt="My Profile Photo">
                    <h1 class="panel-cover__title panel-title">Deniz Engin</h1>

                    <hr class="panel-cover__divider panel-cover__divider--secondary">
                    <a href="#about">
                        <p class="panel-cover__description">About</p>
                    </a>
                    <br>
                    <a href="#news">
                        <p class="panel-cover__description">News</p>
                    </a>
                    <br>
                    <a href="#research">
                        <p class="panel-cover__description">Research</p>
                    </a>


                    <hr class="panel-cover__divider panel-cover__divider--secondary">

                    <div class="navigation-wrapper">

                        <nav class="cover-navigation navigation--social">
                            <ul class="navigation">


                                <!-- Twitter -->
                                <li class="navigation__item">
                                    <a href="https://twitter.com/enginde_" title="@Deniz Engin on Twitter"
                                       target="_blank">
                                        <i class="icon icon-social-twitter"></i>
                                        <span class="label">Twitter</span>
                                    </a>
                                </li>

                                <!-- LinkedIn -->
                                <li class="navigation__item">
                                    <a href="https://www.linkedin.com/in/denizenginn" title="Deniz Engin on LinkedIn"
                                       target="_blank">
                                        <i class="icon icon-social-linkedin"></i>
                                        <span class="label">LinkedIn</span>
                                    </a>
                                </li>


                                <!-- GitHub -->
                                <li class="navigation__item">
                                    <a href="https://www.github.com/engindeniz" title="Deniz Engin on GitHub"
                                       target="_blank">
                                        <i class="icon icon-social-github"></i>
                                        <span class="label">GitHub</span>
                                    </a>
                                </li>


                                <!-- Email -->
                                <li class="navigation__item">
                                    <a href="mailto:deniz.engin@inria.fr" title="Email to Deniz Engin" target="_blank">
                                        <i class="icon icon-mail"></i>
                                        <span class="label">Email</span>
                                    </a>
                                </li>


                            </ul>
                        </nav>

                    </div>

            </div>

        </div>

        <div class="panel-cover--overlay"></div>
    </div>
</header>


<div class="content-wrapper">
    <div class="content-wrapper__inner">


        <div id="about" class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title">
                        About
                    </h2>
                    <p class="excerpt">
                    <p>I am a PhD student at <a href="https://www-linkmedia.irisa.fr/">LinkMedia</a> team in <a
                            href="https://www.inria.fr/fr/centre-inria-rennes-bretagne-atlantique">INRIA Rennes</a> working with <a
                                href="https://avrithis.net/">Yannis Avrithis</a> and <a
                                href="http://people.rennes.inria.fr/Teddy.Furon">Teddy Furon</a>.
                        I have received my BS and MS degrees from Electronics Engineering at <a href="https://www.itu.edu.tr/ ">Istanbul Technical University.</a> My research is focused on multi-modal video understanding, specifically video question answering.</p>
                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>


        <div id="news" class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title">
                        News
                    </h2>
                    <p class="excerpt">
                    <ul>
                    <li>Dec. 2022 - Serving as a reviewer for <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
                        </li>
                    <li>Nov. 2022 - I visited <a href="https://www.iarai.ac.at/">IARAI</a> for two weeks.
                        </li>
                    <li>Jul. 2022 - I participated in <a href="https://iplab.dmi.unict.it/icvss2022/">ICVSS 2022</a>.
                        </li>
                    <li>Jul. 2022 - I presented <a href="https://arxiv.org/abs/2103.14517">Video
                            QA</a> paper at <a href="https://caprfiap2022.sciencesconf.org/">CAp & RFIAP 2022</a>.
                        </li>
                    <li>May 2022 - Serving as a reviewer for <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
                        </li>
                        <li>Oct. 2021 - I presented <a href="https://arxiv.org/abs/2103.14517">Video
                            QA</a> paper at <a href="https://sites.google.com/view/iccv21clvl/"> ICCV21 Workshop on
                            Closing the Loop Between Vision and Language</a>.
                        </li>

                        <li>Sep. 2021 - I participated in <a href="https://ellis.eu/events/ellis-doctoral-symposium">ELLIS 2021</a>.
                        </li>
                        <li>Aug. 2021 - <a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA">Video
                            QA</a> code released.
                        </li>
                        <li>Jul. 2021 - Paper on <a href="https://engindeniz.github.io/dialogsummary-videoqa">Video
                            QA</a> accepted at <a href="https://iccv2021.thecvf.com/">ICCV 2021</a>.
                        </li>
                        <li>Jul. 2021 - I participated in <a href="https://project.inria.fr/paiss/paiss-2021/">PAISS
                            2021</a>.
                        </li>
                        <li>Mar. 2021 - Preprint released at arXiv: <a href="https://arxiv.org/abs/2103.14517">Video
                            QA</a>.
                        </li>
                        <li>Sep. 2020 - Starting my PhD at INRIA and InterDigital.</li>
                        <li>Apr. 2020 - Paper on <a
                                href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Engin_Offline_Signature_Verification_on_Real-World_Documents_CVPRW_2020_paper.pdf">Signature
                            Verification</a> accepted at <a href="https://vislab.ucr.edu/Biometrics2020/index.php">CVPR
                            2020 Biometrics Workshop</a>.
                        </li>

                    </ul>
                    </p>

                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>


        <h2 id="research" class="post-list__post-title post-title">Research</h2>
        <div class="main-post-list">

            <ol class="post-list">
                <p class="excerpt">
                <p>See <a href="http://scholar.google.com.tr/citations?user=5GQk5LUAAAAJ&amp;hl=tr">Google Scholar</a>
                    for all publications.</p>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div><img src="images/dialogsummary_videoqa.png" height="180px" width="400px"/></div>
                        <div class="right-col">
                            <a href="https://arxiv.org/abs/2103.14517">On the hidden treasure of dialog in video
                                question answering
                            </a>
                            <br/>
                            <strong>Deniz Engin</strong>,<a href="https://sites.google.com/site/francoisschnitzler/">François
                            Schnitzler</a>, <a href="https://www.interdigital.com/talent/?id=88">Ngoc Q. K. Duong</a>,
                            <a href="https://avrithis.net/">Yannis Avrithis</a>
                            <br/>
                            <em>ICCV, 2021</em><br/>
                            <a href="https://engindeniz.github.io/dialogsummary-videoqa">project page</a> |
                            <a href="https://arxiv.org/abs/2103.14517">arXiv</a> |
                            <a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA">code</a> | <a
                                class="abstract">abstract</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>High-level understanding of stories in video such as movies and TV shows from raw data is
                                extremely challenging. Modern video question answering (VideoQA) systems often use
                                additional human-made sources like plot synopses, scripts, video descriptions or
                                knowledge bases. In this work, we present a new approach to understand the whole story
                                without such external sources. The secret lies in the dialog: unlike any prior work, we
                                treat dialog as a noisy source to be converted into text description via dialog
                                summarization, much like recent methods treat video. The input of each modality is
                                encoded by transformers independently, and a simple fusion method combines all
                                modalities, using soft temporal attention for localization over long inputs. Our model
                                outperforms the state of the art on the KnowIT VQA dataset by a large margin, without
                                using question-specific human annotation or human-made plot summaries. It even
                                outperforms human evaluators who have never watched any whole episode before. Code is
                                available at https://engindeniz.github.io/dialogsummary-videoqa</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>


                    <div class="two-col-row">
                        <div>
                            <img src="images/signatureVerification.png" height="350px" width="400px"/>
                        </div>
                        <div class="right-col">
                            <a href="https://arxiv.org/abs/2004.12104">Offline Signature Verification on Real-World
                                Documents</a>
                            <br/><strong>Deniz Engin*</strong>, Alperen Kantarcı*, Seçil Arslan, <a
                                href="https://web.itu.edu.tr/ekenel/">Hazım Kemal Ekenel</a><br/>
                            <em>CVPR Biometrics Workshop, </em>2020<br/>
                            <a href="https://arxiv.org/abs/2004.12104">arXiv</a> | <a class="abstract">abstract</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>Research on offline signature verification has explored a large variety of methods on
                                multiple signature datasets, which are collected under controlled conditions. However,
                                these datasets may not fully reflect the characteristics of the signatures in some
                                practical use cases. Real-world signatures extracted from the formal documents may
                                contain different types of occlusions, for example, stamps, company seals, ruling lines,
                                and signature boxes. Moreover, they may have very high intra-class variations, where
                                even genuine signatures resemble forgeries. In this paper, we address a real-world
                                writer independent offline signature verification problem, in which, a bank’s customers’
                                transaction request documents that contain their occluded signatures are compared with
                                their clean reference signatures. Our proposed method consists of two main components, a
                                stamp cleaning method based on CycleGAN and signature representation based on CNNs. We
                                extensively evaluate different verification setups, fine-tuning strategies, and
                                signature representation approaches to have a thorough analysis of the problem.
                                Moreover, we conduct a human evaluation to show the challenging nature of the problem.
                                We run experiments both on our custom dataset, as well as on the publicly available
                                Tobacco-800 dataset. The experimental results validate the difficulty of offline
                                signature verification on real-world documents. However, by employing the stamp cleaning
                                process, we improve the signature verification performance significantly.</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div><img src="images/cycle.png" height="180px" width="400px"/></div>
                        <div class="right-col">
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Engin_Cycle-Dehaze_Enhanced_CycleGAN_CVPR_2018_paper.pdf">Cycle-Dehaze:
                                Enhanced CycleGAN for Single Image Dehazing</a>
                            <br/>
                            <strong>Deniz Engin*</strong>, Anıl Genç*, <a href="https://web.itu.edu.tr/ekenel/">Hazım
                            Kemal Ekenel</a>
                            <br/>
                            <em>CVPR NTIRE Workshop, </em>2018<br/>
                            <a href="https://arxiv.org/abs/1805.05308">arXiv</a> |
                            <a href="https://github.com/Deniz.Enginniz/Cycle-Dehaze">code</a> | <a class="abstract">abstract</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>In this paper, we present an end-to-end network, called Cycle-Dehaze, for single image
                                dehazing problem, which does not require pairs of hazy and corresponding ground truth
                                images for training. That is, we train the network by feeding clean and hazy images in
                                an unpaired manner. Moreover, the proposed approach does not rely on estimation of the
                                atmospheric scattering model parameters. Our method enhances CycleGAN formulation by
                                combining cycle-consistency and perceptual losses in order to improve the quality of
                                textural information recovery and generate visually better haze-free images. Typically,
                                deep learning models for dehazing take low resolution images as input and produce low
                                resolution outputs. However, in the NTIRE 2018 challenge on single image dehazing, high
                                resolution images were provided. Therefore, we apply bicubic downscaling. After
                                obtaining low-resolution outputs from the network, we utilize the Laplacian pyramid to
                                upscale the output images to the original resolution. We conduct experiments on
                                NYU-Depth, I-HAZE, and O-HAZE datasets. Extensive experiments demonstrate that the
                                proposed approach improves CycleGAN method both quantitatively and qualitatively.</p>
                            </p>
                        </div>
                    </div>

                </li>

                <li>
                    <p class="excerpt">
                    <div class="two-col-row">
                        <div>
                            <img src="images/posenormalization.png" height="210px" width="400px"/>
                        </div>
                        <div class="right-col">
                            <br/><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553087">Face
                            Frontalization for Cross-Pose Facial Expression Recognition</a>
                            <br/>
                            <strong>Deniz Engin</strong>, Christophe Ecabert, <a href="https://web.itu.edu.tr/ekenel/">Hazım
                            Kemal Ekenel</a>, <a href="https://people.epfl.ch/jean-philippe.thiran">Jean-Philippe
                            Thiran</a>
                            <br/>
                            <em>EUSIPCO, </em>2018<br/>
                            <a class="abstract">abstract</a>
                        </div>
                    </div>
                    </p>

                    <div class="abstract">
                        <div style="width:50%">
                        </div>
                        <div class="right-col">
                            <p class="excerpt">
                            <p>In this paper, we have explored the effect of pose normalization for cross-pose facial
                                expression recognition. We have first presented an expression preserving face
                                frontalization method. After face frontalization step, for facial expression
                                representation and classification, we have employed both a traditional approach, by
                                using hand-crafted features, namely local binary patterns, in combination with support
                                vector machine classification and a relatively more recent approach based on
                                convolutional neural networks. To evaluate the impact of face frontalization on facial
                                expression recognition performance, we have conducted cross-pose, subject-independent
                                expression recognition experiments using the BU3DFE database. Experimental results show
                                that pose normalization improves the performance for cross-pose facial expression
                                recognition. Especially, when local binary patterns in combination with support vector
                                machine classifier is used, since this facial expression representation and
                                classification does not handle pose variations, the obtained performance increase is
                                significant. Convolutional neural networks-based approach is found to be more successful
                                handling pose variations, when it is fine-tuned on a dataset that contains face images
                                with varying pose angles. Its performance is further enhanced by benefiting from face
                                frontalization.</p>
                            </p>
                        </div>
                    </div>

                </li>

            </ol>


        </div>


    </div>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script type="text/javascript" src="js/main.js?1589893399693513343"></script>


</div>
</body>
</html>