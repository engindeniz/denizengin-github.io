<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="robots" content="noindex">

    <title>Deniz Engin</title>
    <meta name="description" content="VideoQA">
    <meta name="author" content="Deniz Engin">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Deniz Engin">
    <meta property="og:description" content="VideoQA">

    <meta name="msapplication-TileColor" content="#ffc40d">

    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="css/main.css?1589893399693513343">
    <link rel="canonical" href="http://localhost:4000">
    <link rel="alternate" type="application/rss+xml" title="Deniz Engin" href="feed.xml">
</head>


<body>

<div class="publication-content-wrapper">
    <div class="content-wrapper__inner">
        <h2 class="post-list__post-title post-title header-article"
            style="text-align: center; font-size: 22pt; font-weight: bold; padding-bottom: 10px">On
            the hidden treasure of dialog in video question
            answering</h2>
        <div class="main-post-list">
            <ol class="post-list">
                <li>
                    <div class="top-authors" style="padding-bottom: 15px">
                        <div style="margin: auto">
                            <div style="text-align: center"><a href="https://engindeniz.github.io">Deniz Engin</a></div>
                        </div>
                        <div style="margin: auto">
                            <div style="text-align: center"><a href="https://sites.google.com/site/francoisschnitzler/">Fran√ßois
                                Schnitzler</a></div>
                        </div>
                        <div style="margin: auto">
                            <div style="text-align: center"><a href="https://www.interdigital.com/talent/?id=88">Ngoc Q.
                                K. Duong</a></div>
                        </div>
                        <div style="margin: auto">
                            <div style="text-align: center"><a href="https://avrithis.net/">Yannis Avrithis</a></div>
                        </div>
                    </div>
                </li>
                <!--                  </ol>-->
                <!--                <ol>-->
                <li>
                    <div class="two-col-row top-authors">
                        <div style="margin: auto"><a href="https://arxiv.org/abs/2103.14517">Paper</a></div>
                        <div style="margin: auto"><a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA">Code</a></div>
                    </div>
                </li>
                <!--                <li>-->
            </ol>
        </div>


        <div class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title header-article">
                        Abstract
                    </h2>
                    <p style="text-align: justify">High-level understanding of stories in video such as movies and TV
                        shows from raw data is
                        extremely challenging. Modern video question answering (VideoQA) systems often use
                        additional human-made sources like plot synopses, scripts, video descriptions or
                        knowledge bases. In this work, we present a new approach to understand the whole story
                        without such external sources. The secret lies in the dialog: unlike any prior work, we
                        treat dialog as a noisy source to be converted into text description via dialog
                        summarization, much like recent methods treat video. The input of each modality is
                        encoded by transformers independently, and a simple fusion method combines all
                        modalities, using soft temporal attention for localization over long inputs. Our model
                        outperforms the state of the art on the KnowIT VQA dataset by a large margin, without
                        using question-specific human annotation or human-made plot summaries. It even
                        outperforms human evaluators who have never watched any whole episode before.</p>

                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>
        <h2 class="post-list__post-title post-title header-article">
            Method Overview
        </h2>
        <div class="two-col-row" style="display: contents">
            <div>
                <img src="images/dialogsummary_videoqa.png" style="padding-bottom: 20px"/>
            </div>

            <p style="text-align: justify">Our VideoQA system converts dialog and video inputs to episode dialog
                summaries and video descriptions, respectively. Converted inputs and dialog are processed independently
                in streams, along with the question and each answer, producing a score per answer. Finally, stream
                embeddings are fused separately per answer and a
                prediction is made.
            </p>
        </div>
        <hr class="post-list__divider">
        <div class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title header-article">
                        Citation
                    </h2>
                    <div>
                        <code style="border: black;background: none;padding: 0">

                                    <pre style="border: #c7c7c7;border-style: solid;padding: 10px;white-space: pre-wrap">
@inproceedings{engin2021hidden,
    title={On the hidden treasure of dialog in video question answering},
    author={Engin, Deniz and Schnitzler, Fran{\c{c}}ois and Duong, Ngoc QK and Avrithis, Yannis},
    journal={ICCV},
    year={2021}
}
                                    </pre>
                        </code>
                    </div>
                    <hr class="post-list__divider">
                </li>

            </ol>

            <hr class="post-list__divider ">

        </div>
        <div class="main-post-list">

            <ol class="post-list">

                <li>
                    <h2 class="post-list__post-title post-title header-article">
                        Acknowledgements
                    </h2>
                    <p style="text-align: justify">This work was supported by the European Commission under
                        European Horizon 2020 Programme,
                        grant number 951911 - AI4Media. This work was granted access to the HPC resources of
                        IDRIS under the allocation 2020-AD01101226
                        3 made by GENCI.</p>

                    <hr class="post-list__divider">
                </li>

            </ol>


        </div>
    </div>
</div>
</body>
</html>